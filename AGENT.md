0 . Purpose & high-level picture
This repo is an experimental PyTorch re-implementation of DiBS: Differentiable Bayesian Structure Learning for differentiable Bayesian causal discovery.
The JAX reference code (~4 k LOC, mixed model + inference) is flattened here into two Python files so it is easier to iterate, debug, and eventually grow into a modular library:

file	role	key objects / functions
models/dibs_torch_v2.py	Core model + maths (â‰ˆ1.1 k LOC)	scores, gumbel_soft_gmat, acyclic_constr, log_full_likelihood, log_joint, grad_log_joint â€¦
tests/test_inference.py	Smoke-test & playground (â‰ˆ350 LOC)	Generates a 3-node chain X1â†’X2â†’X3, runs plain gradient ascent for 100 steps, logs diagnostics via Hydra

Everything else (Hydra config files, utils, notebooks) is scaffolding.
SVGD, multi-particle inference and advanced DAG-penalty variants are not yet ported â€“ the current loop is deliberately simple so we can validate gradients, clipping, annealing, etc.

1 . How the code hangs together
text
Copy
Edit
[gradient loop in test_inference.py]
â””â”€â”€ builds params  { z, Î¸, t }
    â”‚
    â”œâ”€â”€ log_joint()        (forward value)
    â”‚     â”œâ”€ log_full_likelihood()
    â”‚     â”œâ”€ log_theta_prior()
    â”‚     â””â”€ gumbel_acyclic_constr_mc()
    â”‚          â””â”€ gumbel_soft_gmat() â†’ acyclic_constr()
    â”‚
    â””â”€â”€ grad_log_joint()   (top-level autograd)
          â”œâ”€ grad_z_log_joint_gumbel()
          â”‚     â”œâ”€ gumbel_grad_acyclic_constr_mc()
          â”‚     â””â”€ log_full_likelihood() + log_theta_prior()
          â””â”€ grad_theta_log_joint()
                â””â”€ same two likelihood/prior paths
Z latent ([D, K, 2]) encodes per-edge logits with the Innvae / Fortuin uâ€“v factorisation (Eq. 9 in the paper).

Î¸ is the (currently linear-SEM) weight matrix.

gumbel_soft_gmat() draws differentiable Gumbel-sigmoid edge samples; bernoulli_soft_gmat() produces element-wise probabilities.

acyclic_constr() implements the NOTEARS trace constraint h(G)=tr((I+Î±G)^d)-d. (Batch-aware patch included below.)

Annealing: update_dibs_hparams() rescales Î± and Î² each global step (t) with a warm-up factor so the DAG prior starts gentle and ramps up over time.

2 . Current experiment (chain Xâ‚â†’Xâ‚‚â†’Xâ‚ƒ)
Ground truth:

makefile
Copy
Edit
X1   ~ N(0,1)
X2   = +2.0 Â· X1   + Îµ,  Îµ~N(0,ÏƒÂ²)
X3   = -1.5 Â· X2   + Îµ
Learning loop: 100 iterations of gradient ascent, step-sizes 0.005 for both Z and Î¸, gradient-norm clipping (11 and 19 by default).

Outcome: converges to the correct DAG within 50â€“70 iterations; coefficient estimates within ~10 % of ground truth.

3 . Known pain-points & open questions
ğŸ”§ Symptom	Likely cause	Mitigation ideas
Exploding Î¸ gradients when true coefficients are large (â‰ˆ10 or â‰ˆ-5).	Loss surface becomes stiff; Gaussian prior Ïƒ too small.	Either raise theta_prior_sigma or switch optimiser to Adam with per-parameter adaptivity; tune a â€œwarm startâ€ period for Î².
Learnt graph cyclic for some initialisations.	DAG penalty zero-grad due to torch.bernoulli in gumbel_acyclic_constr_mc (non-diff).	Use soft Gumbel matrix inside h(G) (patch below).
trace dim-3 error beyond iter 80 (reported).	Batched graph input to acyclic_constr, but function assumed 2-D.	Add batch-aware trace (see patch).
Hydra config noise: many unused flags / comments.	Prototype still in flux.	Cull dead keys; move defaults into conf/config.yaml; add CI that runs pytest -q.